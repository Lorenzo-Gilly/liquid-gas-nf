#!/usr/bin/python
#
# Copyright 2022 DeepMind Technologies Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Coupling flows for particle models."""

import functools
from typing import Any, Callable, Mapping, Sequence, Union

import chex
import distrax
from lorenzo_models import bijectors
from lorenzo_models import embeddings
from lorenzo_models import utils
import haiku as hk
import jax
import jax.numpy as jnp

Array = chex.Array
PRNGKey = Array


def _reshape_last(x: Array, ndims: int, new_shape: Sequence[int]) -> Array:
  """Reshapes the last `ndims` dimensions of `x` to shape `new_shape`."""
  if ndims <= 0:
    raise ValueError(
        f'Number of dimensions to reshape must be positive, got {ndims}.')
  return jnp.reshape(x, x.shape[:-ndims] + tuple(new_shape))


def make_equivariant_conditioner(
    shape_transformed: Sequence[int],
    num_bijector_params: int,
    lower: float,
    upper: float,
    embedding_size: int,
    conditioner_constructor: Callable[..., Any],
    conditioner_kwargs: Mapping[str, Any],
    num_frequencies: int,
    w_init_final: hk.initializers.Initializer = hk.initializers.RandomNormal(stddev=0.01),
) -> Callable[[Array], Array]:
  """Make a permutation-equivariant conditioner for the coupling flow."""
  # This conditioner assumes that the input is of shape [..., N, D1]. It returns
  # an output of shape [..., N, D2, K], where:
  #   D2 = `shape_transformed[-1]`
  #   K = `num_bijector_params`
  output_size = shape_transformed[-1]
  conditioner = conditioner_constructor(**conditioner_kwargs)
  return hk.Sequential([
      functools.partial(
          embeddings.circular, lower=lower, upper=upper,
          num_frequencies=num_frequencies),
      hk.Linear(embedding_size),
      conditioner,
      hk.Linear(output_size * num_bijector_params, w_init=w_init_final),
      functools.partial(
          _reshape_last, ndims=1, new_shape=(output_size, num_bijector_params)),
  ])


def make_split_coupling_flow(
    event_shape: Sequence[int],
    lower: float,
    upper: float,
    num_layers: int,
    num_bins: int,
    conditioner: Mapping[str, Any],
    permute_variables: bool,
    split_axis: int,
    use_circular_shift: bool,
    prng: Union[int, PRNGKey],
    circular_shift_init: hk.initializers.Initializer = jnp.zeros,
    ) -> distrax.Bijector:
  """Create a flow that consists of a sequence of split coupling layers.

  All coupling layers use rational-quadratic splines. Each layer of the flow
  is composed of two split coupling bijectors, where each coupling bijector
  transforms a different part of the input.

  The flow maps to and from the range `[lower, upper]`, obeying periodic
  boundary conditions.

  Args:
    event_shape: the shape of the event generated by the flow. Does not
      include the batch dimensions.
    lower: lower range of the flow.
    upper: upper range of the flow.
    num_layers: the number of layers to use. Each layer consists of two split
      coupling bijectors, where each coupling bijector transforms a different
      part of the input.
    num_bins: number of bins to use in the rational-quadratic splines.
    conditioner: a Mapping containing 'constructor' and 'kwargs' keys that
      configures the conditioner used in the coupling layers.
    permute_variables: whether to permute the dimensions along the splitting
      axis between successive layers.
    split_axis: a negative int that defines which axis to split along.
    use_circular_shift: if True, add a learned circular shift between successive
      flow layers.
    prng: either a PRNG key, or an integer seed to convert to a PRNG key. The
      PRNG key will be used to construct the permutations, when these are
      random.
    circular_shift_init: initializer for the circular shifts.

  Returns:
    The flow, a Distrax bijector.
  """
  if isinstance(prng, int):
    prng = jax.random.PRNGKey(prng)

  if split_axis >= 0:
    raise ValueError(f'Expected split axis to be negative, got {split_axis}.')

  def bijector_fn(params: Array):
    return distrax.RationalQuadraticSpline(
        params,
        range_min=lower,
        range_max=upper,
        boundary_slopes='circular',
        min_bin_size=(upper - lower) * 1e-4)

  split_size = event_shape[split_axis]
  split_index = split_size // 2
  cyclic_permutation = jnp.roll(jnp.arange(split_size), shift=1)
  block_ndims = len(event_shape) + split_axis
  layers = []
  for _ in range(num_layers):
    sublayers = []

    # Variable permutation.
    if permute_variables:
      if split_size <= 3:
        permutation = cyclic_permutation
      else:
        prng, key = jax.random.split(prng)
        permutation = jax.random.permutation(key, jnp.arange(split_size))
      # Pure-distrax permute (replaces tfp.bijectors.Permute)
      inv_permutation = jnp.zeros_like(permutation)
      inv_permutation = inv_permutation.at[permutation].set(jnp.arange(split_size))
      permute_layer = distrax.Lambda(
          forward=lambda x, p=permutation: jnp.take(x, p, axis=split_axis),
          inverse=lambda y, ip=inv_permutation: jnp.take(y, ip, axis=split_axis),
          forward_log_det_jacobian=lambda x: jnp.zeros(x.shape[:-(len(event_shape))]),
          inverse_log_det_jacobian=lambda y: jnp.zeros(y.shape[:-(len(event_shape))]),
          event_ndims_in=len(event_shape),
          is_constant_jacobian=True)
      sublayers.append(permute_layer)

    # Circular shift.
    if use_circular_shift:
      shift = utils.Parameter(
          name='circular_shift',
          param_name='shift',
          shape=event_shape[-1:],
          init=circular_shift_init)()
      shift_layer = bijectors.CircularShift(
          (upper - lower) * shift, lower, upper)
      shift_layer = distrax.Block(shift_layer, len(event_shape))
      sublayers.append(shift_layer)

    # Coupling layers.
    for swap in [True, False]:
      shape_transformed = list(event_shape)
      shape_transformed[split_axis] = (
          split_index if swap else split_size - split_index)
      coupling_layer = distrax.SplitCoupling(
          swap=swap,
          split_index=split_index,
          split_axis=split_axis,
          event_ndims=len(event_shape),
          bijector=bijector_fn,
          conditioner=conditioner['constructor'](
              shape_transformed=shape_transformed,
              num_bijector_params=3 * num_bins + 1,
              lower=lower,
              upper=upper,
              **conditioner['kwargs']))
      sublayers.append(coupling_layer)
    layers.append(distrax.Chain(sublayers))

  return distrax.Chain(layers)


def make_type_aware_conditioner(
    shape_transformed: Sequence[int],
    num_bijector_params: int,
    lower: float,
    upper: float,
    embedding_size: int,
    conditioner_constructor: Callable[..., Any],
    conditioner_kwargs: Mapping[str, Any],
    num_frequencies: int,
    *,
    num_type_a: int,                                # how many A’s (rest are B’s)
    type_embedding_size: int | None = None,
    type_ids: Array | Callable[[], Array] | None = None,  # ← NEW: pass tids or a getter
    w_init_final: hk.initializers.Initializer = hk.initializers.RandomNormal(stddev=0.01),
) -> Callable[[Array], Array]:
  """Permutation-equivariant conditioner that is *type-aware* via embeddings.

  Input: [..., N, D_in]  (pass-through half).
  Output: [..., N, D_out, K] with D_out = shape_transformed[-1],
          K = num_bijector_params (per transformed coord).
  """
  output_size = shape_transformed[-1]
  e_type = type_embedding_size or max(16, embedding_size // 8)

  transformer = conditioner_constructor(**conditioner_kwargs)

  def _features(x: Array) -> Array:
    # 1) Positional features (unchanged).
    pos_feat = embeddings.circular(
        x, lower=lower, upper=upper, num_frequencies=num_frequencies
    )  # [..., N, F_pos]

    # 2) Get per-particle type ids aligned with the current particle order.
    if callable(type_ids):
      tids = type_ids()                                     # [..., N]
    elif type_ids is not None:
      tids = type_ids                                       # [..., N]
    else:
      N = x.shape[-2]
      base = (jnp.arange(N) >= num_type_a).astype(jnp.int32)     # [N]: 0..0,1..1
      tids = jnp.broadcast_to(base, x.shape[:-2] + (N,))         # [..., N]

    # 3) Tiny learned embedding per type id, then concat with position features.
    type_embed = hk.Embed(vocab_size=2, embed_dim=e_type)(tids)  # [..., N, e_type]
    return jnp.concatenate([pos_feat, type_embed], axis=-1)      # [..., N, F_pos+e_type]

  return hk.Sequential([
      _features,
      hk.Linear(embedding_size),
      transformer,
      hk.Linear(output_size * num_bijector_params, w_init=w_init_final),
      functools.partial(_reshape_last, ndims=1,
                        new_shape=(output_size, num_bijector_params)),
  ])